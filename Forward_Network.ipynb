{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Forward_Network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM+eCpz4Sfj7RoIQiNSH/pm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shineloveyc/Doing_ML/blob/master/Forward_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZzg5tF1VsCT",
        "colab_type": "text"
      },
      "source": [
        "## Multilayer perceptro using Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwsQQyMcVlRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#https://pytorch.org/docs/stable/nn.functional.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4f8K1-MV2GB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultilayerPerceptron(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    \"\"\" Args:\n",
        "            initiatiate the model\n",
        "            input_dim (int): the size of the input vectors\n",
        "            hidden_dim (int): the output size of the first Linear layer \n",
        "            output_dim (int): the output size of the second Linear layer\n",
        "\"\"\"\n",
        "\n",
        "    super(MultilayerPerceptron, self).__init__() #inherent the nn.Module class\n",
        "    #define linear function with input and output\n",
        "    self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "    self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "  #define the forward function\n",
        "  def forward(self, x_in, apply_softmax = False):\n",
        "    \"\"\"The forward pass of the MLP\n",
        "    Args:\n",
        "        x_in (torch.Tensor): an input data tensor\n",
        "        x_in.shape should be (batch, input_dim) \n",
        "        apply_softmax (bool): a flag for the softmax activation\n",
        "          should be false if used with the cross-entropy losses \n",
        "        Returns:\n",
        "          the resulting tensor. tensor.shape should be (batch, output_dim) \"\"\"\n",
        "     #add non-linear function in between two linear functions     \n",
        "    intermediate = F.relu(self.fc1(x_in))\n",
        "    output = self.fc2(intermediate)\n",
        "\n",
        "    if apply_softmax:\n",
        "      output = F.softmax(output,dim =1)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W6-mMYlay2l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b2c04a41-16ad-4345-c942-28da2d8f582b"
      },
      "source": [
        "#instantiation of an MLP\n",
        "batch_size = 2 #number of samples input at once\n",
        "input_dim = 3 #the node of the first layer\n",
        "hidden_dim =100 # the node of the hidden layer\n",
        "output_dim = 4 #the node of the final layer\n",
        "\n",
        "#Initialize model\n",
        "mlp = MultilayerPerceptron(input_dim, hidden_dim, output_dim)\n",
        "print(mlp)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MultilayerPerceptron(\n",
            "  (fc1): Linear(in_features=3, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH8du8Fdb82u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "38035421-156f-4208-b8cd-32aeebe577f2"
      },
      "source": [
        "#Testing the MLP with random inputs\n",
        "\n",
        "def describe(x):\n",
        "  print(\"type : {}\".format(x.type()))\n",
        "  print('shape/size: {}'.format(x.shape))\n",
        "  print('values: \\n{}'.format(x))\n",
        "\n",
        "x_input = torch.rand(batch_size,input_dim)\n",
        "describe(x_input)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "type : torch.FloatTensor\n",
            "shape/size: torch.Size([2, 3])\n",
            "values: \n",
            "tensor([[0.8113, 0.3775, 0.1514],\n",
            "        [0.5811, 0.9488, 0.4711]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfjOwgGtdTbB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "1ab98021-d795-4c28-8143-ecfa5104093e"
      },
      "source": [
        "#mlp starting to train by deploy the forward function, so the arg should match what in the forward function\n",
        "# the input dim should be (batch, input_dim)\n",
        "y_output = mlp(x_input, apply_softmax = False)\n",
        "\n",
        "#the output size should be (batch, output_dim)\n",
        "#2 is the number of data points in the minibatch\n",
        "# y the is final feature vectors(in some setting it is a predictor vector) for each data point\n",
        "describe(y_output)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "type : torch.FloatTensor\n",
            "shape/size: torch.Size([2, 4])\n",
            "values: \n",
            "tensor([[-0.1795,  0.1133,  0.4556, -0.0532],\n",
            "        [-0.1123,  0.1245,  0.4992, -0.0041]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "946nBRngfeur",
        "colab_type": "text"
      },
      "source": [
        "However, if you want to turn the prediction vector into probabilities, an extra step is required. Specifically, you require the softmax activation function, which is used to transform a vector of values into probabilities. The softmax function has many roots. In physics, it is known as the Boltzmann or Gibbs distribution; in statistics, it’s multinomial logistic regression; and in the natural language processing (NLP) community it’s known as the maximum entropy (MaxEnt) classifier.\n",
        "<br>\n",
        "Whateverthename,theintuitionunderlyingthefunctionisthatlarge positive values will result in higher probabilities, and lower negative values will result in smaller probabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSW_GQ7PfiTj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a09a9ec3-b1b3-4a48-f269-611b72138806"
      },
      "source": [
        "#Producing probabilistic outputs with a MLP classifier\n",
        "y_output = mlp(x_input, apply_softmax = True)\n",
        "describe(y_output)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "type : torch.FloatTensor\n",
            "shape/size: torch.Size([2, 4])\n",
            "values: \n",
            "tensor([[0.1865, 0.2499, 0.3520, 0.2116],\n",
            "        [0.1914, 0.2425, 0.3528, 0.2133]], grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbrkdHVDgtQf",
        "colab_type": "text"
      },
      "source": [
        "To conclude, MLPs are stacked Linear layers that map tensors to other tensors. \n",
        "<br>\n",
        "* Nonlinearities are used between each pair of Linear layers to break the linear\n",
        "relationship and allow for the model to twist the vector space around. \n",
        "* In a classification setting, this twisting should result in linear separability between classes. \n",
        "* Additionally, you can use the softmax function to interpret MLP outputs as probabilities, but you should not use softmax with specific loss functions, because the underlying implementations can leverage superior mathematical/computational shortcuts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWAYtXhjg3mh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}